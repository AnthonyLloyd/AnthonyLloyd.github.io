---
layout: post
title: "Data-First Architecture - Asset Management"
tags: [functional,architecture,data]
description: "Data-First Architecture - Asset Management"
keywords: functional, architecture, data
exclude: true
---
<p>I recently had a light bulb moment when I saw a <a href="https://twitter.com/etodd_/status/936587511580844032">tweet</a> from Evan Todd.
It helped bring together some ideas I've had for a while on software architecture.</p>
<blockquote>
<p>Data characteristics excluding software functionality should dictate the system architecture.</p>
</blockquote>
<p>The shape, size and rate of change of the data is the most important factor when starting to architect a system.
The first thing that needs to be done is estimate these characteristics in average and extreme cases.</p>
<p>Functional programming encourages this mindset since the data and functions are kept separate.
Data is simple and pure functions are simple. Combining them leads to needless complexity.</p>
<p>I'm going to make the case with an example.
I will argue that most asset management systems store and use the wrong data.
This limits functionality and increases the complexity of these systems.</p>
<img style="border:1px solid black" src="/{{site.baseurl}}public/twitter/to_sum_up.png" title="To sum up"/>
<h2>Traditional Approach</h2>
<p>Most asset management systems consider <code>positions</code>, <code>profit</code> and <code>returns</code> to be their main data.
You can see this as they normally have an overnight batch process that generates and saves <code>positions</code> for the next day.</p>
<p>This produces an enormous amount of duplicated data.
Databases are large and grow rapidly.
What is being saved is essentially a chosen set of calculation results.</p>
<p>What's worse is that other data processes are built on the top of this position data such as adjustments, lock-down and fund aggregation.</p>
<p>I think this architecture comes from not investigating the characteristics of the data first and jumping straight to thinking about system entities and functionality.</p>
<h2>Data-First Approach</h2>
<p>The primary data for asset management is <code>trades</code>, asset <code>terms</code> and <code>time series</code>.
All other data is just calculations based these.
We can ignore these for now and consider caching of results at a later stage.</p>
<p><code>Terms</code> data is complex in structure but relatively small in size and changes infrequently. Event sourcing works well here for audit and a changing schema.
<code>Time series</code> data is simple in structure and can be efficiently compressed down to 10-20% of its original size.
<code>Trades</code> data is a simple list of asset quantities from one entity to another.
The data is effectively all numeric and fixed size.
An append only ledger style structure works well here.</p>
<p>We can use the <a href="https://www.ishares.com/uk/intermediaries/en/products/etf-product-list#!type=emeaIshares&amp;tab=overview&amp;view=list">iShares</a> fund range as a fairly extreme example.
Downloading these funds over a period of time and analysing the data gives us some useful statistics.</p>
<p>280 funds, 100-1000 positions per fund, 1000 trades per year per fund. Per day?
from 4, to 4, instrument 4, quantity 4, flow type 2, trade date 8, settle days 2, trade id 8, usertime 8 ~ 50 bytes
Data size for a trade is 50 bytes * 5 flows = 256 bytes.
Fund for 1 year 250 KB
Fund for 10 years 2.4 MB
280 funds for 10 years 700 MB</p>
<p>Now we have a feel for the data we can start to make some decisions about the architecture.</p>
<p>Given the size we can decide to load and cache by whole fund.
This will simplify the code and give us greater flexibilty on the various types of profit and return measures we can offer.
The majority of these calculations are ideally done as a single pass through the ordered trades.
It turns out with in memory data this is a negligable processing cost and can just be done on screen refresh.</p>
<p>We can also look at a hierarchy of funds and perform the calculations at a parent level.
Since most of the data is append only we can keep a cache saved (encryped of course) on the client to further save cloud costs.</p>
<h2>Conclusion</h2>
<p>People are suprised when I say you can just hold this data in memory.
Infinitely scalable by default leads to bad perf + complexity.</p>
<p>In the days of cloud computing where architectural costs are more obvious right sizing the architecture to the data is more important.</p>
<p>Most articles titled architecture jump straight in to some feature of the codebase.</p>
<p>So we can build a system that is simpler, faster, more flexible and cheaper to run because we first understood the data.</p>
<img style="border:1px solid black" src="/{{site.baseurl}}public/twitter/10_servers.png" title="10 Servers"/>
<h2>Todo</h2>
<p>Because data cost many powers of 10 more time to retrieve. And also data shape is a constant in the system. Code changes.</p>
<p>We are not google, our extreme cases will be easier to estimate.</p>
<p>To wrap up: think about the actual problem and the data it needs. Then write functions to manage that data. Don't think about classes and interfaces and closures and reflection and RAII and exceptions and polymorphism and who knows what else.</p>
<h2>References</h2>
<p><a href="http://etodd.io/2015/09/28/one-weird-trick-better-code/">The One Weird Trick: data first, not code first - Even Todd</a><br />
<a href="https://news.ycombinator.com/item?id=10291688">Data first, not code first - Hacker News</a><br />
<a href="http://gamedevs.org/uploads/practical-examples-in-data-oriented-design.pdf">Practical Examples in Data Oriented Design - Niklas Frykholm</a><br />
<a href="https://martinfowler.com/articles/lmax.html#QueuesAndTheirLackOfMechanicalSympathy">Queues and their lack of mechanical sympathy - Martin Fowler</a></p>


